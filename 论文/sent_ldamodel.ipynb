{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "需要注意的是情感大分类这个地方和我们模型中的情感的个数是不一致的，所有的词语都可以分为这7种类型\n",
    "但是我们在进行情感分析的时候有的时候只需要其中的几类就可以比如说是，只研究情感极性\n",
    "或者只研究喜怒哀乐这4个方面，在使用模型的时候还得看情况使用\n",
    "大分类：按照文件来的\n",
    "极性：褒义、贬义、中性、间有  0:中性，1：褒义，2：贬义，3：兼有\n",
    "情感态度：正面 反面： 0：正面  1：反面\n",
    "情感表达：喜怒哀乐  1--表示喜  2----表示怒，3------表示哀 4 ------表示乐\n",
    "\n",
    "需要注意的是我们找到在训练LDA模型的时候，我们通过其情感大分类则可以知道其相应的情感大分类——表达\n",
    "以及情感分类——态度   ，无须保留\n",
    "这些值如何更新？？？？我们在训练的时候可以利用词典赋予新的情感标签（如果能找到的话则使用若找不到，则随机赋予），但是在采样过程中，\n",
    "得到了新的情感标签的时候，我们可以要更新这个词在词库里的信息，最后进行保存，查看区别！！！！\n",
    "对于那些没有的词我们也要保留其信息，进行后期验证\n",
    "强度没有办法进行更新只能每次相应的修改\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('G:/anconada/envs/py36/lib/site-packages')\n",
    "from prettytable import PrettyTable\n",
    "import re \n",
    "import jieba\n",
    "import os\n",
    "import copy\n",
    "from zhon.hanzi import punctuation\n",
    "from scipy.misc import imread\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "import gc\n",
    "import time\n",
    "from gensim.models  import word2vec\n",
    "from sklearn.feature_extraction.text  import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from guppy import hpy\n",
    "import json\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "\n",
    "''' 在写论文的时候可以使用两组数据查看模型的效果'''\n",
    "\n",
    "class ldamodel():\n",
    "    def __init__(self,topic_num,sentiment_num,alpha,beta,gamma,corpus,interation,final_info_sentword,df_info):\n",
    "        self.T=topic_num\n",
    "        self.D=None\n",
    "        self.S=sentiment_num\n",
    "        self.V=None\n",
    "\n",
    "        #这两个词典用来指定词语的情感标签，若我们想要评论的喜怒哀乐（此时S=4）则使用df_info 中的情感大分类_表达作为该词的情感标签\n",
    "        #若想考查评论的态度（即正面评论还是反面评论，此时S=2）,则使用df_info中的情感大分类_态度作为该词的情感标签\n",
    "\n",
    "        self.sentiment_dict=final_info_sentword\n",
    "        self.sentiment_map=df_info\n",
    "\n",
    "        #超参数设置\n",
    "        self.alpha=alpha if alpha else 0.1\n",
    "        self.beta= beta if beta else 0.1\n",
    "        self.gamma=gamma if gamma else 0.1 \n",
    "        self.corpus=corpus\n",
    "        self.interation= interation if interation else 1000\n",
    "\n",
    "        self.word2id=None\n",
    "        self.id2word=None\n",
    "\n",
    "        #设置参数\n",
    "        self.doc_sel_topic_count=None\n",
    "        self.topic_sel_word_count=None\n",
    "        self.doc_count=None\n",
    "        self.topic_count=None\n",
    "        self.sentiment_count=None\n",
    "        self.topic_sentiment_count=None\n",
    "        self.doc_sentiment_count=None\n",
    "        \n",
    "\n",
    "        self.doc_sel_topic=None\n",
    "        self.topic_sel_word=None\n",
    "        self.doc_sel=None\n",
    "        \n",
    "        self.z=None\n",
    "        self.l=None\n",
    "\n",
    "        if self.S==4:\n",
    "            print('我们开始查看评论的情感表达情况--喜怒哀乐')\n",
    "        elif self.S==2:\n",
    "            print('我们开始考查评论的情感态度--正面或者反面')\n",
    "        elif self.S==7:\n",
    "            print('我们开始考查评论的情感大类情况---乐、好、怒、哀、惧、恶、惊')\n",
    "\n",
    "    def createdictionary(self,cut_corpus):\n",
    "        word2id=dict()\n",
    "        wordnum=0\n",
    "        cut_doc_id = copy.deepcopy(cut_corpus)\n",
    "        for i , doc in enumerate(cut_corpus):\n",
    "            for j,word in enumerate(doc):\n",
    "                wordnum+=1   #记录了所有单词的个数\n",
    "                if word not in word2id.keys():\n",
    "                    word2id[word]=len(word2id)\n",
    "                cut_doc_id[i][j]=word2id[word]\n",
    "        self.V=wordnum\n",
    "        self.D=len(cut_corpus)\n",
    "        self.word2id=word2id\n",
    "        self.id2word=dict(zip(word2id.values(),word2id.keys()))\n",
    "        return word2id,dict(zip(word2id.values(),word2id.keys())),cut_doc_id,wordnum \n",
    "    \n",
    "      \n",
    "  \n",
    "    def initial(self,cut_doc_id):\n",
    "        self.doc_sel_topic_count=np.zeros([self.D,self.S,self.T])\n",
    "        self.topic_sel_word_count=np.zeros([self.S,self.T,self.V])\n",
    "        self.doc_count=np.zeros(self.D)\n",
    "        self.sentiment_count=np.zeros(self.S)  ##这个不是必须的\n",
    "        self.topic_count=np.zeros(self.T)      ##这个不是必须的\n",
    "        self.topic_sentiment_count=np.zeros([self.T,self.S])\n",
    "        self.doc_sentiment_count=np.zeros([self.D,self.S])\n",
    "\n",
    "        self.doc_sel_topic=np.ndarray([self.D,self.S,self.T])\n",
    "        self.topic_sel_word=np.ndarray([self.S,self.T,self.V])\n",
    "        self.doc_sel=np.ndarray([self.D,self.S])\n",
    "        \n",
    "        self.z=np.zeros([self.D,self.V])  #存放每个文档中每一个词的主题\n",
    "        self.l=np.zeros([self.D,self.V])  #存放每个文档中每个词的情感极性\n",
    "\n",
    "        for i,doc in enumerate(cut_doc_id):\n",
    "            for j,word_id in enumerate(doc):\n",
    "                word=self.id2word(word_id)\n",
    "                topic=int(random.randint(0,self.T-1))\n",
    "                senti_dalei=self.sentiment_dict[word][0] #需要注意的是这个地方随着查看的情感的不同需要一直改变\n",
    "                if self.S==7:\n",
    "                    sentiment=int(senti_dalei)\n",
    "                elif self.S==4:\n",
    "                    sentiment=int(self.sentiment_map[sentiment_map['情感大类']==senti_dalei]['情感大分类_表达'])\n",
    "                elif self.S==4:\n",
    "                    sentiment=int(self.sentiment_map[sentiment_map['情感大类']==senti_dalei]['情感大分类_态度'])\n",
    "                else:\n",
    "                    sentiment=random.randint(0,self.S-1)\n",
    "                self.doc_sel_topic_count[i,sentiment,topic]+=1\n",
    "                self.topic_sel_word_count[sentiment,topic,word_id]+=1\n",
    "                self.doc_count[i]+=1\n",
    "                self.sentiment_count[sentiment]+=1\n",
    "                self.topic_count[topic]+=1\n",
    "                self.topic_sentiment_count[topic,sentiment]+=1\n",
    "                self.doc_sentiment_count[i,sentiment]+=1\n",
    "                \n",
    "                self.z[i,word_id]=topic\n",
    "                self.l[i,word_id]=sentiment\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def gibbssampling(self,cut_doc_id):\n",
    "        for iter in range(self.interation):\n",
    "            for i,doc in enumerate(cut_doc_id):\n",
    "                for j,word_id in enumerate(doc):\n",
    "                    topic=int(self.z[i,word_id])\n",
    "                    sentiment=int(self.l[i,word_id])\n",
    "              \n",
    "                    n_jkd=self.doc_sel_topic_count[i,sentiment,topic]-1\n",
    "                    n_jkw=self.topic_sel_word_count[sentiment,topic,word_id]-1\n",
    "                    n_jk=self.topic_sentiment_count[topic,sentiment]-1\n",
    "                    n_kd=self.doc_sentiment_count[i,sentiment]-1\n",
    "                    n_d=self.doc_count[i]-1 \n",
    "                                       \n",
    "                    new_topic,new_sentiment=self.resampling(n_jkd,n_jkw,n_jk,n_kd,n_d)\n",
    "\n",
    "                    self.z[i,word_id]=new_topic\n",
    "                    self.l[i,word_id]=new_sentiment\n",
    "              \n",
    "                    self.doc_sel_topic_count[i,sentiment,topic]-=1\n",
    "                    self.topic_sel_word_count[sentiment,topic,word_id]-=1\n",
    "                    self.topic_sentiment_count[topic,sentiment]-=1\n",
    "                    self.doc_sentiment_count[i,sentiment]-=1\n",
    "                    self.topic_count[topic]-=1\n",
    "                    self.sentiment_count[sentiment]-=1\n",
    "              \n",
    "\n",
    "                    self.doc_sel_topic_count[i,new_sentiment,new_topic]+=1\n",
    "                    self.topic_sel_word_count[new_sentiment,new_topic,word_id]+=1\n",
    "                    self.topic_sentiment_count[new_topic,new_sentiment]+=1\n",
    "                    self.doc_sentiment_count[i,new_sentiment]+=1\n",
    "                    self.topic_count[new_topic]+=1\n",
    "                    self.sentiment_count[new_sentiment]+=1\n",
    "            print('已经迭代到了第{0}次了'.format(iter+1))\n",
    "            \n",
    "        self.updateparam()\n",
    "    ###验证模型\n",
    "\n",
    "\n",
    "    def updateparam(self):\n",
    "        for i in range(self.D):\n",
    "            for j in range(self.S):\n",
    "                self.doc_sel[i,j]=(self.doc_sentiment_count[i,j] + self.gamma)/(self.doc_count[i] + self.S * self.gamma)\n",
    "\n",
    "        for i in range(self.S):\n",
    "            for  j in range(self.T):\n",
    "                for k in range(self.V):\n",
    "                    self.topic_sel_word[i,j,k]=(self.topic_sel_word_count[i,j,k] + self.beta)/(\n",
    "                        self.topic_sentiment_count[j,i] + self.beta * self.V)\n",
    "        for i in range(self.D):\n",
    "            for j in range(self.S):\n",
    "                for k in range(self.T):\n",
    "                    self.doc_sel_topic[i,j,k]=(self.doc_sel_topic_count[i,j,k] + self.alpha)/(\n",
    "                        self.doc_sentiment_count[i,j] + self.T * self.alpha)\n",
    "        print('参数更新完成******************* \\n')\n",
    "        return\n",
    "\n",
    "    \n",
    "                    \n",
    " \n",
    "    def resampling(self,n_jkd,n_jkw,n_jk,n_kd,n_d):  \n",
    "        pk = np.ndarray([self.T,self.S]) \n",
    "        for i in range(self.T):\n",
    "            for j in range(self.S):\n",
    "                pk[i,j] = float(n_jkd + self.alpha)*(n_jkw +self.beta)*(n_kd + self.gamma)/(\n",
    "                  (n_kd + self.alpha*self.T)*(n_jk + self.beta*self.V)*(n_d + self.gamma * self.S))\n",
    "                if i>0 and j>0:\n",
    "                    pk[i,j]+=pk[i,j-1]\n",
    "        # 轮盘方式随机选择主题\n",
    "        u = random.random()*pk[self.T-1,self.S-1]\n",
    "        for j in range(self.T):\n",
    "            for  k in range(self.S):\n",
    "                if pk[j,k]>=u:\n",
    "                    #print('get the new topic {0} and new sentiment {1}'.format(j,k))\n",
    "                    return j,k\n",
    "\n",
    "    def predict(self,new_doc,word2id,isupdate=False):\n",
    "        '''\n",
    "            predict:new doc / comment\n",
    "        '''\n",
    "        #对新文档进行切分等处理\n",
    "\n",
    "        #获取新文档中在word2id中存在的单词\n",
    "        new_doc_id=list()\n",
    "        for word in new_doc:\n",
    "            if word in word2id:\n",
    "                new_doc_id.append(word2id[word])\n",
    "        \n",
    "        #参数的设置  涉及到文档的矩阵需要重新设置一个新的，其余的不变\n",
    "        new_dstc=np.zeros([1,self.S,self.T]) \n",
    "        new_dsc=np.zeros([1,self.S])\n",
    "        new_dc=0\n",
    "        new_tswc=copy.deepcopy(self.topic_sel_word_count)\n",
    "        new_sc=copy.deepcopy(self.sentiment_count)\n",
    "        new_tsc=copy.deepcopy(self.topic_sentiment_count)\n",
    "        new_tc=copy.deepcopy(self.topic_count)\n",
    "        \n",
    "        new_z=np.zeros([1,self.V])\n",
    "        new_l=np.zeros([1,self.V])\n",
    "\n",
    "        #参数的更新，和之前的过程类似\n",
    "        for i,word_id in enumerate(new_doc_id):\n",
    "            topic=int(self.z[0,word_id])\n",
    "            sentiment=int(self.l[0,word_id])\n",
    "            \n",
    "            new_dstc[0,sentiment,topic]+=1\n",
    "            new_dsc[0,sentiment]+=1\n",
    "            new_dc+=1\n",
    "            new_tswc[sentiment,topic,word_id]+=1\n",
    "            new_sc[sentiment]+=1\n",
    "            new_tsc[topic,sentiment]+=1\n",
    "            new_tc[topic]+=1\n",
    "\n",
    "            new_z[0,word_id]=topic\n",
    "            new_l[0,word_id]=sentiment\n",
    "            \n",
    "        \n",
    "       \n",
    "        \n",
    "        #开始进行采样了\n",
    "        for iter in range(0,self.interation):\n",
    "            for word_id in new_doc_id:\n",
    "                topic=int(new_z[0,word_id])\n",
    "                sentiment=int(new_l[0,word_id])\n",
    "                \n",
    "                n_jkd=new_dstc[0,sentiment,topic]-1\n",
    "                n_jkw=new_tswc[sentiment,topic,word_id]-1\n",
    "                n_jk=new_tsc[sentiment,topic]-1\n",
    "                n_kd=new_dsc[0,sentiment]-1\n",
    "                n_d=new_dc-1\n",
    "\n",
    "                #此处需要进行重新给每个该单词进行重新赋予主题\n",
    "                new_topic,new_sentiment=self.resampling(n_jkd,n_jkw,n_jk,n_kd,n_d)   \n",
    "\n",
    "                #更新旧的新的topic的值\n",
    "                new_dstc[0,sentiment,topic]-=1 \n",
    "                new_dsc[0,sentiment]-=1\n",
    "                new_tswc[sentiment,topic,word_id]-=1\n",
    "                new_sc[sentiment]-=1\n",
    "                new_tsc[sentiment,topic]-=1\n",
    "                new_tc[topic]-=1\n",
    "\n",
    "                new_dstc[0,new_sentiment,new_topic]+=1 \n",
    "                new_dsc[0,new_sentiment]+=1\n",
    "                new_tswc[new_sentiment,new_topic,word_id]+=1\n",
    "                new_sc[new_sentiment]+=1\n",
    "                new_tsc[new_sentiment,new_topic]+=1\n",
    "                new_tc[new_topic]+=1\n",
    "\n",
    "                new_z[0,word_id]=new_topic\n",
    "                new_l[0,word_id]=new_sentiment\n",
    "\n",
    "            if (iter+1)%100==0:\n",
    "                print('new_doc 第{0}次训练'.format(iter+1))\n",
    "                #此时要输出LDA模型的评价标准\n",
    "                  \n",
    "        if isupdate==True:\n",
    "            self.topic_sel_word_count=new_tswc\n",
    "            self.sentiment_count=new_sc\n",
    "            self.topic_sentiment_count=new_tsc\n",
    "            self.topic_count=new_tc\n",
    "            self.doc_sel_topic_count=np.r_[self.doc_sel_topic_count,new_dstc]\n",
    "            self.doc_sentiment_count=np.r_[self.doc_sentiment_count,new_dsc]\n",
    "            self.doc_count=np.r_[self.doc_count,new_dc]\n",
    "            self.updateparam()\n",
    "            print('加载new_doc之后选择更新参数，并更新完成')\n",
    "        else:\n",
    "            print('选择不更新参数')\n",
    "        print('输出参数')\n",
    "        print(new_dstc)\n",
    "        print(new_tswc)\n",
    "        print(new_dsc)\n",
    "        print(new_dc)\n",
    "        print(new_tc)\n",
    "        print(new_tsc)\n",
    "        print(new_sc)\n",
    "        return [new_dstc,new_tswc,new_dsc,new_dc,new_tc,new_tsc,new_sc]\n",
    "\n",
    "    def get_top_word(self,topnums=20):\n",
    "        '''打印出来每个主题与其概率最高词语的组合--等式\n",
    "    将每一个topic的高频单词读取出来并保存'''\n",
    "        with open('./content/top_word','w') as f:\n",
    "          for i in range(0,self.K):\n",
    "            top_words=np.argsort(self.topic_word[i,:])[:topnums]\n",
    "            top_word=[self.id2word[j] for j in top_words]\n",
    "            top_words = '\\t'.join(top_words)\n",
    "            res = 'topic{0}: \\t {1}'.format(i, top_words)\n",
    "            f.write(res+'\\n')\n",
    "            #print(res)\n",
    "  \n",
    "    def get_top_topic(self,topicnums=20,wordnums=20):\n",
    "        with open('./concent/top_topic_word','w') as f:\n",
    "            for doc in range(self.D):\n",
    "                top_topic=np.argsort(self.doc_topic[doc,:])[:topicnums]\n",
    "                res='doc:{0}\\t'.format(doc)\n",
    "                f.write(res)\n",
    "                for theam in top_topic:\n",
    "                  topword=np.argsort(self.topic_word[theam,:])[:wordnums]\n",
    "                  topword=[self.id2word[j] for j in topword ]\n",
    "                  re='\\t'.join(topword)\n",
    "                  res='topic:{0} \\t {1}'.format(theam,re)\n",
    "                  f.write(re+'\\n')\n",
    "        f.close()\n",
    "        return \n",
    "\n",
    "    def print_topic_word(self,doc_id,topic_list,word_nums=20):\n",
    "        all_num=len(topic_list)\n",
    "        table=PrettyTable()\n",
    "        for i in topic_list:\n",
    "          topword=np.argsort(self.topic_word[i,:])[:word_nums]\n",
    "          table.add_column(i, [self.id2word[jj] for jj in topword])\n",
    "        print(table)\n",
    "        \n",
    "        #打印出来该文档上的主题分布以及在每个主题上面的个数的图形\n",
    "        doc_topic_count=self.doc_topic_count[doc_id,:]\n",
    "        sns.stripplot(x=list(range(0,all_num-1)),y=doc_topic_count)\n",
    "        for i in topic_list:\n",
    "          sns.scatterplot(x=range(0,self.V-1),y=self.topic_word[i,:])\n",
    "          plt.show()\n",
    "          sns.countplot(x=range(0,self.V-1),hue=self.topic_word[i,:])\n",
    "          plt.show()\n",
    "\n",
    "    \n",
    "  \n",
    "if __name__=='__main__':\n",
    "    stopwords_path='../论文/中文停用词/stopwords'\n",
    "    path='C:/Users/Administrator/Desktop/data/评论/cut_comment_1.txt'\n",
    "    all_text=[]\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            lines=line.strip().split(' ')\n",
    "            all_text.append(lines)\n",
    "        f.close()\n",
    "    comment_train, comment_test = train_test_split(all_text, test_size = 0.1)\n",
    "    path_1 = 'C:/Users/Administrator/Desktop/data/评论/final_info_sentword.txt'\n",
    "    f = open(path_1, 'r', encoding='utf-8')\n",
    "    data = f.read()\n",
    "    test = re.sub('\\'', '\\\"', data)\n",
    "    test= test.lstrip('\\ufeff')\n",
    "    final_info_sentword = json.loads(test)\n",
    "    path_2='C:/Users/Administrator/Desktop/data/评论/df_info.csv'\n",
    "    df_info=pd.read_csv(path_2,engine='python')\n",
    "\n",
    "\n",
    "    M=ldamodel(20,5,0.1,0.1,0.1,comment_train,100,final_info_sentword=final_info_sentword,df_info=df_info)\n",
    "    word2id,id2word,cut_corpus_id,wordnum=M.createdictionary(comment_train)\n",
    "\n",
    "    info = psutil.virtual_memory()\n",
    "\n",
    "    print('没有运行initial 之前的内存使用情况')\n",
    "    print(u'内存使用：', psutil.Process(os.getpid()).memory_info().rss)\n",
    "    print(u'总内存：', info.total)\n",
    "    print(u'内存占比：', info.percent)\n",
    "    print(u'cpu个数：', psutil.cpu_count())\n",
    "\n",
    "    M.initial(cut_corpus_id)\n",
    "    start=time.time()\n",
    "    info = psutil.virtual_memory()\n",
    "\n",
    "    print('没有运行Gibbs sampling 之前的内存使用情况')\n",
    "    print(u'内存使用：', psutil.Process(os.getpid()).memory_info().rss)\n",
    "    print(u'总内存：', info.total)\n",
    "    print(u'内存占比：', info.percent)\n",
    "    print(u'cpu个数：', psutil.cpu_count())\n",
    "\n",
    "    M.gibbssampling(cut_corpus_id)\n",
    "    end=time.time()\n",
    "    print('gibbssampling stage use {0} second'.format(end-start))\n",
    "    test0=comment_test[0]\n",
    "    M.predict(test0,word2id)\n",
    "\n",
    "    print('运行Gibbs Sampling 之后的内存使用情况')\n",
    "    print(u'内存使用：', psutil.Process(os.getpid()).memory_info().rss)\n",
    "    print(u'总内存：', info.total)\n",
    "    print(u'内存占比：', info.percent)\n",
    "    print(u'cpu个数：', psutil.cpu_count())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
