{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from prettytable import PrettyTable\n",
    "import re\n",
    "import jieba\n",
    "import os\n",
    "import copy\n",
    "from zhon.hanzi import punctuation\n",
    "from scipy.misc import imread\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "import gc\n",
    "import time\n",
    "from gensim.models  import word2vec\n",
    "from sklearn.feature_extraction.text  import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sys.path.append('G:/anconada/envs/py36/lib/site-packages')\n",
    "\n",
    "\n",
    "class ldamodel():\n",
    "    def __init__(self, topic_num, sentiment_num, alpha, beta, gamma, corpus, interation):\n",
    "        self.T = topic_num\n",
    "        self.D = None\n",
    "        self.S = sentiment_num\n",
    "        self.V = None\n",
    "\n",
    "        # 超参数设置\n",
    "        self.alpha = alpha if alpha else 0.1\n",
    "        self.beta = beta if beta else 0.1\n",
    "        self.gamma = gamma if gamma else 0.1\n",
    "        self.corpus = corpus\n",
    "        self.interation = interation if interation else 1000\n",
    "\n",
    "        # 设置参数\n",
    "        self.doc_sel_topic_count = None\n",
    "        self.topic_sel_word_count = None\n",
    "        self.doc_count = None\n",
    "        self.topic_count = None\n",
    "        self.sentiment_count = None\n",
    "        self.topic_sentiment_count = None\n",
    "        self.doc_sentiment_count = None\n",
    "\n",
    "        self.doc_sel_topic = None\n",
    "        self.topic_sel_word = None\n",
    "        self.doc_sel = None\n",
    "\n",
    "        self.z = None\n",
    "        self.l = None\n",
    "\n",
    "\n",
    "    def createdictionary(self, cut_corpus):\n",
    "        word2id = dict()\n",
    "        wordnum = 0\n",
    "        cut_doc_id = copy.deepcopy(cut_corpus)\n",
    "        for i, doc in enumerate(cut_corpus):\n",
    "            for j, word in enumerate(doc):\n",
    "                wordnum += 1  # 记录了所有单词的个数\n",
    "                if word not in word2id.keys():\n",
    "                    word2id[word] = len(word2id)\n",
    "                cut_doc_id[i][j] = word2id[word]\n",
    "        self.V = wordnum\n",
    "        self.D = len(cut_corpus)\n",
    "        return word2id, dict(zip(word2id.values(), word2id.keys())), cut_doc_id, wordnum\n",
    "\n",
    "\n",
    "    def initial(self, cut_doc_id):\n",
    "        self.doc_sel_topic_count = np.zeros([self.D, self.S, self.T])\n",
    "        self.topic_sel_word_count = np.zeros([self.S, self.T, self.V])\n",
    "        self.doc_count = np.zeros(self.D)\n",
    "        self.sentiment_count = np.zeros(self.S)  ##这个不是必须的\n",
    "        self.topic_count = np.zeros(self.T)  ##这个不是必须的\n",
    "        self.topic_sentiment_count = np.zeros([self.T, self.S])\n",
    "        self.doc_sentiment_count = np.zeros([self.D, self.S])\n",
    "\n",
    "        self.doc_sel_topic = np.ndarray([self.D, self.S, self.T])\n",
    "        self.topic_sel_word = np.ndarray([self.S, self.T, self.V])\n",
    "        self.doc_sel = np.ndarray([self.D, self.S])\n",
    "\n",
    "        self.z = np.zeros([self.D, self.V])  # 存放每个文档中每一个词的主题\n",
    "        self.l = np.zeros([self.D, self.V])  # 存放每个文档中每个词的情感极性\n",
    "\n",
    "        for i, doc in enumerate(cut_doc_id):\n",
    "            for j, word_id in enumerate(doc):\n",
    "                topic = random.randint(0, self.T - 1)\n",
    "                sentiment = random.randint(0, self.S - 1)\n",
    "\n",
    "                self.doc_sel_topic_count[i, sentiment, topic] += 1\n",
    "                self.topic_sel_word_count[sentiment, topic, word_id] += 1\n",
    "                self.doc_count[i] += 1\n",
    "                self.sentiment_count[sentiment] += 1\n",
    "                self.topic_count[topic] += 1\n",
    "                self.topic_sentiment_count[topic, sentiment] += 1\n",
    "                self.doc_sentiment_count[i, sentiment] += 1\n",
    "\n",
    "                self.z[i, word_id] = topic\n",
    "                self.l[i, word_id] = sentiment\n",
    "\n",
    "\n",
    "    def gibbssampling(self, cut_doc_id):\n",
    "        for iter in range(self.interation):\n",
    "            for i, doc in enumerate(cut_doc_id):\n",
    "                for j, word_id in enumerate(doc):\n",
    "                    topic = int(self.z[i, word_id])\n",
    "                    sentiment = int(self.l[i, word_id])\n",
    "\n",
    "                    n_jkd = self.doc_sel_topic_count[i, sentiment, topic] - 1\n",
    "                    n_jkw = self.topic_sel_word_count[sentiment, topic, word_id] - 1\n",
    "                    n_jk = self.topic_sentiment_count[topic, sentiment] - 1\n",
    "                    n_kd = self.doc_sentiment_count[i, sentiment] - 1\n",
    "                    n_d = self.doc_count[i] - 1\n",
    "\n",
    "                    new_topic, new_sentiment = self.resampling(n_jkd, n_jkw, n_jk, n_kd, n_d)\n",
    "\n",
    "                    self.z[i, word_id] = new_topic\n",
    "                    self.l[i, word_id] = new_sentiment\n",
    "\n",
    "                    self.doc_sel_topic_count[i, sentiment, topic] -= 1\n",
    "                    self.topic_sel_word_count[sentiment, topic, word_id] -= 1\n",
    "                    self.topic_sentiment_count[topic, sentiment] -= 1\n",
    "                    self.doc_sentiment_count[i, sentiment] -= 1\n",
    "                    self.topic_count[topic] -= 1\n",
    "                    self.sentiment_count[sentiment] -= 1\n",
    "\n",
    "                    self.doc_sel_topic_count[i, new_sentiment, new_topic] += 1\n",
    "                    self.topic_sel_word_count[new_sentiment, new_topic, word_id] += 1\n",
    "                    self.topic_sentiment_count[new_topic, new_sentiment] += 1\n",
    "                    self.doc_sentiment_count[i, new_sentiment] += 1\n",
    "                    self.topic_count[new_topic] += 1\n",
    "                    self.sentiment_count[new_sentiment] += 1\n",
    "            print('已经迭代到了第{0}次了'.format(iter + 1))\n",
    "\n",
    "        self.updateparam()\n",
    "\n",
    "\n",
    "###验证模型\n",
    "\n",
    "\n",
    "    def updateparam(self):\n",
    "        for i in range(self.D):\n",
    "            for j in range(self.S):\n",
    "                self.doc_sel[i, j] = (self.doc_sentiment_count[i, j] + self.gamma) / (\n",
    "                            self.doc_count[i] + self.S * self.gamma)\n",
    "\n",
    "        for i in range(self.S):\n",
    "            for j in range(self.T):\n",
    "                for k in range(self.V):\n",
    "                    self.topic_sel_word[i, j, k] = (self.topic_sel_word_count[i, j, k] + self.beta) / (\n",
    "                            self.topic_sentiment_count[j, i] + self.beta * self.V)\n",
    "        for i in range(self.D):\n",
    "            for j in range(self.S):\n",
    "                for k in range(self.T):\n",
    "                    self.doc_sel_topic[i, j, k] = (self.doc_sel_topic_count[i, j, k] + self.alpha) / (\n",
    "                            self.doc_sentiment_count[i, j] + self.T * self.alpha)\n",
    "        print('参数更新完成******************* \\n')\n",
    "        return\n",
    "\n",
    "\n",
    "    def resampling(self, n_jkd, n_jkw, n_jk, n_kd, n_d):\n",
    "        pk = np.ndarray([self.T, self.S])\n",
    "        for i in range(self.T):\n",
    "            for j in range(self.S):\n",
    "                pk[i, j] = float(n_jkd + self.alpha) * (n_jkw + self.beta) * (n_kd + self.gamma) / (\n",
    "                        (n_kd + self.alpha * self.T) * (n_jk + self.beta * self.V) * (n_d + self.gamma * self.S))\n",
    "                if i > 0 and j > 0:\n",
    "                    pk[i, j] += pk[i, j - 1]\n",
    "        # 轮盘方式随机选择主题\n",
    "        u = random.random() * pk[self.T - 1, self.S - 1]\n",
    "        for j in range(self.T):\n",
    "            for k in range(self.S):\n",
    "                if pk[j, k] >= u:\n",
    "                    # print('get the new topic {0} and new sentiment {1}'.format(j,k))\n",
    "                    return j, k\n",
    "\n",
    "\n",
    "    def predict(self, new_doc, word2id, isupdate=False):\n",
    "        '''\n",
    "            predict:new doc / comment\n",
    "        '''\n",
    "        # 对新文档进行切分等处理\n",
    "\n",
    "        # 获取新文档中在word2id中存在的单词\n",
    "        new_doc_id = list()\n",
    "        for word in new_doc:\n",
    "            if word in word2id:\n",
    "                new_doc_id.append(word2id[word])\n",
    "\n",
    "        # 参数的设置  涉及到文档的矩阵需要重新设置一个新的，其余的不变\n",
    "        new_dstc = np.zeros([1, self.S, self.T])\n",
    "        new_dsc = np.zeros([1, self.S])\n",
    "        new_dc = 0\n",
    "        new_tswc = copy.deepcopy(self.topic_sel_word_count)\n",
    "        new_sc = copy.deepcopy(self.sentiment_count)\n",
    "        new_tsc = copy.deepcopy(self.topic_sentiment_count)\n",
    "        new_tc = copy.deepcopy(self.topic_count)\n",
    "\n",
    "        new_z = np.zeros([1, self.V])\n",
    "        new_l = np.zeros([1, self.V])\n",
    "\n",
    "        # 参数的更新，和之前的过程类似\n",
    "        for i, word_id in enumerate(new_doc_id):\n",
    "            topic = int(self.z[0, word_id])\n",
    "            sentiment = int(self.l[0, word_id])\n",
    "\n",
    "            new_dstc[0, sentiment, topic] += 1\n",
    "            new_dsc[0, sentiment] += 1\n",
    "            new_dc += 1\n",
    "            new_tswc[sentiment, topic, word_id] += 1\n",
    "            new_sc[sentiment] += 1\n",
    "            new_tsc[topic, sentiment] += 1\n",
    "            new_tc[topic] += 1\n",
    "\n",
    "            new_z[0, word_id] = topic\n",
    "            new_l[0, word_id] = sentiment\n",
    "\n",
    "        # 开始进行采样了\n",
    "        for iter in range(0, self.interation):\n",
    "            for word_id in new_doc_id:\n",
    "                topic = int(new_z[0, word_id])\n",
    "                sentiment = int(new_l[0, word_id])\n",
    "\n",
    "                n_jkd = new_dstc[0, sentiment, topic] - 1\n",
    "                n_jkw = new_tswc[sentiment, topic, word_id] - 1\n",
    "                n_jk = new_tsc[sentiment, topic] - 1\n",
    "                n_kd = new_dsc[0, sentiment] - 1\n",
    "                n_d = new_dc - 1\n",
    "\n",
    "                # 此处需要进行重新给每个该单词进行重新赋予主题\n",
    "                new_topic, new_sentiment = self.resampling(n_jkd, n_jkw, n_jk, n_kd, n_d)\n",
    "\n",
    "                # 更新旧的新的topic的值\n",
    "                new_dstc[0, sentiment, topic] -= 1\n",
    "                new_dsc[0, sentiment] -= 1\n",
    "                new_tswc[sentiment, topic, word_id] -= 1\n",
    "                new_sc[sentiment] -= 1\n",
    "                new_tsc[sentiment, topic] -= 1\n",
    "                new_tc[topic] -= 1\n",
    "\n",
    "                new_dstc[0, new_sentiment, new_topic] += 1\n",
    "                new_dsc[0, new_sentiment] += 1\n",
    "                new_tswc[new_sentiment, new_topic, word_id] += 1\n",
    "                new_sc[new_sentiment] += 1\n",
    "                new_tsc[new_sentiment, new_topic] += 1\n",
    "                new_tc[new_topic] += 1\n",
    "\n",
    "                new_z[0, word_id] = new_topic\n",
    "                new_l[0, word_id] = new_sentiment\n",
    "\n",
    "            if (iter + 1) % 100 == 0:\n",
    "                print('new_doc 第{0}次训练'.format(iter + 1))\n",
    "                # 此时要输出LDA模型的评价标准\n",
    "\n",
    "        if isupdate == True:\n",
    "            self.topic_sel_word_count = new_tswc\n",
    "            self.sentiment_count = new_sc\n",
    "            self.topic_sentiment_count = new_tsc\n",
    "            self.topic_count = new_tc\n",
    "            self.doc_sel_topic_count = np.r_[self.doc_sel_topic_count, new_dstc]\n",
    "            self.doc_sentiment_count = np.r_[self.doc_sentiment_count, new_dsc]\n",
    "            self.doc_count = np.r_[self.doc_count, new_dc]\n",
    "            self.updateparam()\n",
    "            print('加载new_doc之后选择更新参数，并更新完成')\n",
    "        else:\n",
    "            print('选择不更新参数')\n",
    "        print('输出参数')\n",
    "        print(new_dstc)\n",
    "        print(new_tswc)\n",
    "        print(new_dsc)\n",
    "        print(new_dc)\n",
    "        print(new_tc)\n",
    "        print(new_tsc)\n",
    "        print(new_sc)\n",
    "        return [new_dstc, new_tswc, new_dsc, new_dc, new_tc, new_tsc, new_sc]\n",
    "\n",
    "\n",
    "    def get_top_word(self, topnums=20):\n",
    "        '''打印出来每个主题与其概率最高词语的组合--等式\n",
    "    将每一个topic的高频单词读取出来并保存'''\n",
    "        with open('./content/top_word', 'w') as f:\n",
    "            for i in range(0, self.K):\n",
    "                top_words = np.argsort(self.topic_word[i, :])[:topnums]\n",
    "                top_word = [self.id2word[j] for j in top_words]\n",
    "                top_words = '\\t'.join(top_words)\n",
    "                res = 'topic{0}: \\t {1}'.format(i, top_words)\n",
    "                f.write(res + '\\n')\n",
    "                # print(res)\n",
    "\n",
    "\n",
    "    def get_top_topic(self, topicnums=20, wordnums=20):\n",
    "        with open('./concent/top_topic_word', 'w') as f:\n",
    "            for doc in range(self.D):\n",
    "                top_topic = np.argsort(self.doc_topic[doc, :])[:topicnums]\n",
    "                res = 'doc:{0}\\t'.format(doc)\n",
    "                f.write(res)\n",
    "                for theam in top_topic:\n",
    "                    topword = np.argsort(self.topic_word[theam, :])[:wordnums]\n",
    "                    topword = [self.id2word[j] for j in topword]\n",
    "                    re = '\\t'.join(topword)\n",
    "                    res = 'topic:{0} \\t {1}'.format(theam, re)\n",
    "                    f.write(re + '\\n')\n",
    "        f.close()\n",
    "        return\n",
    "\n",
    "\n",
    "    def print_topic_word(self, doc_id, topic_list, word_nums=20):\n",
    "        all_num = len(topic_list)\n",
    "        table = PrettyTable()\n",
    "        for i in topic_list:\n",
    "            topword = np.argsort(self.topic_word[i, :])[:word_nums]\n",
    "            table.add_column(i, [self.id2word[jj] for jj in topword])\n",
    "        print(table)\n",
    "\n",
    "        # 打印出来该文档上的主题分布以及在每个主题上面的个数的图形\n",
    "        doc_topic_count = self.doc_topic_count[doc_id, :]\n",
    "        sns.stripplot(x=list(range(0, all_num - 1)), y=doc_topic_count)\n",
    "        for i in topic_list:\n",
    "            sns.scatterplot(x=range(0, self.V - 1), y=self.topic_word[i, :])\n",
    "            plt.show()\n",
    "            sns.countplot(x=range(0, self.V - 1), hue=self.topic_word[i, :])\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    stopwords_path = '../论文/中文停用词/stopwords'\n",
    "    path = 'C:/Users/Administrator/Desktop/data/评论/cut_comment_1.txt'\n",
    "    all_text = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            lines = line.strip().split(' ')\n",
    "            all_text.append(lines)\n",
    "        f.close()\n",
    "    comment_train, comment_test = train_test_split(all_text, test_size=0.5)\n",
    "\n",
    "    M = ldamodel(20, 5, 0.1, 0.1, 0.1, comment_train, 100)\n",
    "    word2id, id2word, cut_corpus_id, wordnum = M.createdictionary(comment_train)\n",
    "    M.initial(cut_corpus_id)\n",
    "    start = time.time()\n",
    "    M.gibbssampling(cut_corpus_id)\n",
    "    end = time.time()\n",
    "    print('gibbssampling stage use {0} second'.format(end - start))\n",
    "    test0 = comment_test[0]\n",
    "    M.predict(test0, word2id)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
