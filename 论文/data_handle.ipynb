{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('G:/anconada/envs/py36/lib/site-packages')\n",
    "from prettytable import PrettyTable\n",
    "import re \n",
    "import jieba\n",
    "import os\n",
    "import copy\n",
    "from zhon.hanzi import punctuation\n",
    "from scipy.misc import imread\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "class Loaddata():\n",
    "    def __init__(self):\n",
    "        print('开始处理文本数据')\n",
    "        \n",
    "    def _loadstopwords(self,stopwords_path):\n",
    "        '''停用词：融合网络停用词、哈工大停用词、川大停用词'''\n",
    "        stop_words = set()\n",
    "        with open(stopwords_path + u'/中文停用词库.txt','r',encoding='gbk') as fr:\n",
    "            for line in fr.readlines():\n",
    "                item = line.strip().split(' ')\n",
    "                for it in item:\n",
    "                    stop_words.add(it)\n",
    "            print('中文停用词已经加载完成！！！***********')\n",
    "            fr.close()\n",
    "        with open(stopwords_path + u'/哈工大停用词表.txt','r',encoding='gbk') as fr:\n",
    "            for line in fr.readlines():\n",
    "                item = line.strip().split(' ')\n",
    "                for it in item:\n",
    "                    stop_words.add(it)\n",
    "            print('哈工大停用词已经加载完成！！！************')\n",
    "            fr.close()\n",
    "        with open(stopwords_path + u'/四川大学机器智能实验室停用词库.txt','r',encoding='gbk') as fr:\n",
    "            for line in fr.readlines():\n",
    "                item = line.strip().split(' ')\n",
    "                for it in item:\n",
    "                    stop_words.add(it)\n",
    "            print('四川大学实验室停用词加载完成！！！***************')\n",
    "            fr.close()\n",
    "        with open(stopwords_path + u'/百度停用词列表.txt','r',encoding='utf-8') as fr:\n",
    "            for line in fr.readlines():\n",
    "                item = line.strip().split(' ')\n",
    "                for it in item:\n",
    "                    stop_words.add(it)\n",
    "            print('百度停用词已经加载完成！！！******************')\n",
    "        with open(stopwords_path + u'/网络停用词.txt','r',encoding='utf-8') as fr:\n",
    "            for line in fr.readlines():\n",
    "                item = line.strip().split(' ')\n",
    "                for it in item:\n",
    "                    stop_words.add(it)\n",
    "            print('网络停用词加载完成！！！！！************')\n",
    "            fr.close()\n",
    "        stop_words.add('')\n",
    "        stop_words.add(' ')\n",
    "        stop_words.add(u'\\u3000')\n",
    "        stop_words.add(u'日')\n",
    "        stop_words.add(u'月')\n",
    "        stop_words.add(u'时')\n",
    "        stop_words.add(u'分')\n",
    "        stop_words.add(u'秒')\n",
    "        stop_words.add(u'报道')\n",
    "        stop_words.add(u'新闻')\n",
    "        stop_words.add(u'本文')\n",
    "        stop_words.add(u'网易')\n",
    "        stop_words.add(u'记者')\n",
    "        stop_words.add(u'来源')\n",
    "        stop_words.add(u'责任编辑')\n",
    "        stop_words.add(u'王晓易')\n",
    "        stop_words.add(u'新华网')\n",
    "        stop_words.add(u'NE00111')\n",
    "        stop_words.add(u'真是太')\n",
    "        stop_words.add(u'金木水火土')\n",
    "        stop_words.add(u'上次')\n",
    "        stop_words.add(u'始终认为')\n",
    "        stop_words.add(u'评论')\n",
    "        stop_words.add(u'ColorOS')\n",
    "        stop_words.add(u'Aeno')\n",
    "        stop_words.add(u'GPU')\n",
    "        stop_words.add(u'gpu')\n",
    "        stop_words.add(u'VO')\n",
    "        stop_words.add(u' color OS MIUI ')\n",
    "        stop_words.add(u'emui')\n",
    "        stop_words.add(u' color')\n",
    "        stop_words.add(u'OS')\n",
    "        stop_words.add(u'nfc')\n",
    "        stop_words.add(u'O')\n",
    "        stop_words.add(u'hellip')\n",
    "        stop_words.add(u'OTG')\n",
    "        stop_words.add('NFC ')\n",
    "\n",
    "\n",
    "        print('所有的停用词加载完成')\n",
    "        return stop_words\n",
    "\n",
    "    def load_news(self,all_path,type_content):\n",
    "        sentences=[]\n",
    "        root_path='C:/Users/Administrator/Desktop/data/新闻/'\n",
    "        i=0\n",
    "        for path in all_path:\n",
    "            text=[]\n",
    "            f=open(root_path+type_content+'/'+path,'r',encoding='utf-8')\n",
    "            for line in f.readlines():\n",
    "                 lineData=line.strip().split(' ')\n",
    "                 text.extend(lineData)\n",
    "            text.pop(0)\n",
    "            text.pop(0)\n",
    "            text.pop(0)\n",
    "            text.pop(0)\n",
    "            text.pop(-1)\n",
    "            text.pop(-1)\n",
    "            text.pop(-1)\n",
    "            text.pop(-1)\n",
    "            s=''.join(text)\n",
    "            sentences.append(s)\n",
    "            #print('第{0}篇文章'.format(i))\n",
    "            i+=1\n",
    "            f.close()\n",
    "        print('{0}篇{1}文章加载完成'.format(i,type_content))\n",
    "        return sentences\n",
    "    \n",
    "    def load_comment(self,stopwords_path,path):\n",
    "        stop_words=list(self._loadstopwords(stopwords_path))\n",
    "        comment_info=pd.read_csv(path,engine='python')\n",
    "        data_comment=comment_info['comment'].tolist()\n",
    "        corpus=[]\n",
    "        for c in data_comment:\n",
    "            new_c=re.sub(r'[%s,\\t,\\\\]+'%punctuation,' ',c)\n",
    "            cut_c=jieba.lcut(new_c)\n",
    "            new_doc=[]\n",
    "            for word in cut_c:\n",
    "                #print(word,word.isalpha())\n",
    "                if word not in stop_words:\n",
    "                    if word.isalpha() is True :\n",
    "                        new_doc.append(word)\n",
    "                        #print(word)\n",
    "            corpus.append(new_doc)\n",
    "        f=open('C:/Users/Administrator/Desktop/data/评论/cut_comment_1.txt','w',encoding='utf-8')\n",
    "        for i in corpus:\n",
    "            f.write(' '.join(i) )\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "        print('已经加载完毕评论形成corpus***************')\n",
    "        return corpus\n",
    "    def load_comment_zwqgfxylk(self,stopwords_path,path):\n",
    "        root_path='C:/Users/Administrator/Desktop/data/中文情感分析语料库/'\n",
    "        cut_corpus=[]\n",
    "        stop_words = list(self._loadstopwords(stopwords_path))\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            c=[]\n",
    "            for line in f.readlines():\n",
    "                new_c = re.sub(r'[%s,\\t,\\\\]+' % punctuation, ' ', line)\n",
    "                cut_c = jieba.lcut(new_c)\n",
    "                for word in cut_c:\n",
    "                    if word not in stop_words :\n",
    "                        if  word.isalpha() is True:\n",
    "                            c.append(word)\n",
    "            cut_corpus.append(c)\n",
    "            f.close()\n",
    "        return cut_corpus\n",
    "        \n",
    "    def _load_corpus(self,sentences,stopwords_path):\n",
    "        '''得到的 corpus是一个双层列表'''\n",
    "        stop_words=list(self._loadstopwords(stopwords_path))\n",
    "        corpus=[]\n",
    "        for s in sentences:\n",
    "            new_s=re.sub(r'[%s]+'%punctuation, \" \", s)\n",
    "            cut_s=jieba.lcut(new_s)\n",
    "            new_doc=[]\n",
    "            #print('文章 \\n',sentence_cut)\n",
    "            for word in cut_s:\n",
    "                if len(word)==1:\n",
    "                    continue\n",
    "                if word not in stop_words and not word.isdigit():\n",
    "                    new_doc.append(word)\n",
    "            corpus.append(new_doc)\n",
    "        print('文本已经去掉停用词以及数字')\n",
    "        return corpus\n",
    "    def _count_num(self,corpus,top_k=50,low_k=50):\n",
    "        '''\n",
    "        count the number of every word in corpus\n",
    "\n",
    "        return:\n",
    "\n",
    "           print top-k  and low-k  （and print those number by table）\n",
    "\n",
    "           word_count: is a dict,the key is word,the value is the number \n",
    "           \n",
    "        we should use low-k filtration the low-frequency words\n",
    "        \n",
    "        '''\n",
    "        word_count=dict()\n",
    "        for i ,doc in enumerate(corpus):\n",
    "            for j,word in enumerate(doc):\n",
    "                if word not in word_count.keys():\n",
    "                    word_count[word]=word_count.get(word,0)+1\n",
    "                else:\n",
    "                    word_count[word]+=1\n",
    "\n",
    "        sort_word=list(sorted(word_count.items(),key=lambda x :x[1],reverse=True))\n",
    "        top_word=sort_word[:top_k]\n",
    "        low_word=sort_word[-low_k:]\n",
    "        \n",
    "        top_table=PrettyTable(['word','number'])\n",
    "        low_table=PrettyTable(['word','number'])\n",
    "        for i in top_word:\n",
    "            top_table.add_row(i)\n",
    "        for i in low_word:\n",
    "            low_table.add_row(i)\n",
    "\n",
    "        print('the number of top_{0}  word  \\n'.format(top_k))\n",
    "        print(top_table)\n",
    "            \n",
    "        print('the number of low_{0}  word  \\n'.format(low_k))\n",
    "        print(low_table)\n",
    "\n",
    "        return word_count\n",
    "    def load_data(self,sentences,stopwords_path,top_k,low_k):\n",
    "        '''\n",
    "        filter the low-frequency-word\n",
    "        '''\n",
    "        all_text=self._load_corpus(sentences,stopwords_path)\n",
    "        word_count=self._count_num(all_text,top_k=50,low_k=50)\n",
    "        corpus=[] \n",
    "        for i,doc in enumerate(all_text):\n",
    "            corpus.append([])\n",
    "            for j ,word in enumerate(doc):\n",
    "                if word_count[word]==1:\n",
    "                    continue\n",
    "                else:\n",
    "                    corpus[-1].append(word)\n",
    "        return corpus,word_count\n",
    "    def getwordcloud(self,word_count,img_path):\n",
    "        '''\n",
    "            get word-cloud of corpus\n",
    "\n",
    "            word_count : is a matrix of word-frequency\n",
    "            \n",
    "        '''\n",
    "        color_mask = imread(img_path) #读取背景图片，\n",
    "        cloud = WordCloud(font_path=\"simsun.ttc\",mask=color_mask,background_color='white',max_words=400,max_font_size=100,width=1000,\\\n",
    "                          height = 500,margin = 10,prefer_horizontal = 0.8)\n",
    "        # background_color='black'\n",
    "        wc = cloud.generate_from_frequencies(word_count)\n",
    "        #mm=img_path.replace('.jpg','词云.jpg')\n",
    "        #wc.to_file(mm)\n",
    "        image_colors = ImageColorGenerator(color_mask)\n",
    "        plt.imshow(wc)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "class get_dagword():\n",
    "    def __int__(self):\n",
    "        print('得到词袋矩阵')\n",
    "\n",
    "    def getdagword(self,all_text):\n",
    "        '''\n",
    "        return :\n",
    "        \n",
    "            word2id--------is a dict ,key is word ,the value is serial number\n",
    "            \n",
    "            id2word--------is a dict ,key is the serial number of word , the value is word\n",
    "\n",
    "            corpus :is a double list,the element of list represent the serial number of the ith doc and the jth word\n",
    "\n",
    "            wordnum: the size of corpus ,the number of total token\n",
    "\n",
    "        '''\n",
    "        word2id=dict()\n",
    "        wordnum=0\n",
    "        corpus=copy.deepcopy(all_text)\n",
    "        for i ,doc in enumerate(all_text):\n",
    "            for j, word in enumerate(doc):\n",
    "                wordnum+=1\n",
    "                if word not in word2id.keys():\n",
    "                    word2id[word]=len(word2id)\n",
    "                corpus[i][j]=word2id[word]\n",
    "        print('词袋矩阵加载完成**********************')\n",
    "        return word2id,dict(zip(word2id.values(),word2id.keys())),corpus,wordnum\n",
    "\n",
    "'''\n",
    "\n",
    "if __name__=='__main__':\n",
    "    stopwords_path='../论文/中文停用词/stopwords'\n",
    "    root_path='C:/Users/Administrator/Desktop/data/新闻/'\n",
    "    ###现在要加载所有的新闻\n",
    "    type_list=['公益新闻','旅游新闻','娱乐新闻','健康新闻','科技新闻']\n",
    "    P=Loaddata()\n",
    "    all_sentences=[]\n",
    "    for type_content in type_list:\n",
    "        all_path=os.listdir(root_path+type_content+'/')\n",
    "        sentences=P.load_news(all_path,type_content)\n",
    "        all_sentences.extend(sentences)\n",
    "    all_text,word_count=P.load_data(all_sentences,stopwords_path,top_k=50,low_k=50)\n",
    "    img_path='C:/Users/Administrator/Desktop/data/新闻/词云图/公益.jpg'\n",
    "    P.getwordcloud(word_count,img_path)\n",
    "    M=get_dagword()\n",
    "    word2id,id2word,corpus,wordnum=M.getdagword(all_text)\n",
    "'''\n",
    "\n",
    "if __name__=='__main__':\n",
    "    P=Loaddata()\n",
    "    stopwords_path='../论文/中文停用词/stopwords'\n",
    "    path='C:/Users/Administrator/Desktop/data/评论/comment_info_final.csv'\n",
    "    all_text=P.load_comment(stopwords_path,path)\n",
    "    path_list=os.listdir('C:\\\\Users\\\\Administrator\\\\Desktop\\\\data\\\\中文情感分析语料库')\n",
    "    load_comment_zwqgfxylk(stopwords_path, path)\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
